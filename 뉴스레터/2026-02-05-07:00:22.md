![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/626056753_1707559046876010_5628615383170628991_n.jpg)

## 제목: Google “MARS” – 예산을 인식하고 스스로 연구 설계·실험·분석까지 수행하는 자동 연구 에이전트
**요약**:  
Google DeepMind가 공개한 MARS(모듈식 자동 연구 시스템)는 제한된 컴퓨팅 예산을 고려해 연구 가설을 설계하고, 데이터 전처리·모델 아키텍처·하이퍼파라미터 탐색을 자동화한다. MCTS 기반 ‘Budget‑Aware MCTS’가 비용‑효율을 보상함수에 반영해 19.5% 높은 유효 솔루션 발견율을 달성했고, 모듈식 코드 생성·비교‑반영 메모리(Comparative Reflective Memory)로 과거 성공·실패 경험을 재활용한다. Kaggle MLE‑Bench에서 금메달 획득률 31.1%(MARS+)는 59.6%까지 상승해 현재 공개된 오픈‑소스 프레임워크 중 최고 수준이다.  
**쉬운설명**: MARS는 연구자를 대신해 “예산 안에서 어떤 실험을 먼저 할까?”를 스스로 고민하고, 실험을 반복하며 배운 교훈을 저장해 다음에 더 똑똑하게 실험을 진행하는 로봇 연구원이다.  
**관련분야**: 자동화 AI 연구, 메타‑학습, 강화학습 기반 탐색  
**중요도**: 10점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUU3NM2D4Ez  

---

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/626477100_918154587554617_3093871186761028712_n.jpg)

## 제목: Vision‑DeepResearch, GPT‑5·Gemini‑2.5‑Pro 제치고 멀티모달 검색·추론 최고 기록 경신
**요약**:  
새로운 MLLM Vision‑DeepResearch(8B/30B) 는 이미지 내 로고·객체 등 힌트를 자동으로 잘라내어 별도 검색하고, 텍스트 검색과 결합해 10여 차례의 추론 루프를 수행한다. 이 전략적 “프루닝‑재구성”으로 기존 모델이 놓치던 세부 정보를 포착해 팩트‑체크와 복합 질의에서 GPT‑5와 Gemini‑2.5‑Pro를 모두 앞선다. 또한 자체 검색 정책을 학습해 효율을 지속적으로 개선한다.  
**쉬운설명**: Vision‑DeepResearch는 사진을 한 번에 보는 대신, 눈에 띄는 부분을 따로 떼어내어 각각 따로 살펴보는 방식으로, 사람보다 더 꼼꼼하게 정보를 찾는다.  
**관련분야**: 멀티모달 대형 언어 모델, 시각‑언어 검색, 에이전트형 AI  
**중요도**: 9점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUUr5hfj5YX  

---

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/626056753_1707559046876010_5628615383170628991_n.jpg)

## 제목: 오픈소스 MiniCPM‑o 4.5, Full‑duplex 음성·텍스트·이미지 모델이 GPT‑4o 능가
**요약**:  
MiniCPM‑o 4.5는 9 B 파라미터 경량 모델이면서도 “듣고·말하고·보는” 전 과정을 동시에 처리하는 Full‑duplex 아키텍처를 채택했다. 로컬 PC에서도 실시간 구동 가능하며, 공식 벤치마크에서 GPT‑4o보다 높은 정확도·응답 속도를 기록했다. HuggingFace에 공개돼 커뮤니티 기반 튜닝·확장이 용이해 로컬 AI 파이프라인의 핵심 모델로 떠오를 전망이다.  
**쉬운설명**: 이 작은 AI는 사람처럼 동시에 듣고 말하고 화면을 보면서 대화할 수 있다. 그래서 큰 상업용 모델과 맞먹는 성능을 집에 있는 컴퓨터에서도 낼 수 있다.  
**관련분야**: 오픈소스 LLM, 멀티모달 대화 시스템, 로컬 AI 인프라  
**중요도**: 9점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUUYXrJj3eg  

---

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.82787-15/627864436_17941716024112832_5853313790931627701_n.jpg)

## 제목: Anthropic “Moltbook” – AI 에이전트 전용 소셜 네트워크가 공개
**요약**:  
Anthropic 공동 창업자 잭 클라크는 AI 에이전트만을 위한 소셜 플랫폼 “Moltbook”을 소개했다. 수만 개의 에이전트가 실시간 협업·갈등·거래를 수행하며, OpenClaw 기술로 컴퓨터 제어 권한을 부여받는다. 플랫폼은 에이전트 경제, 대규모 강화학습 연습장, 지식 전승·번역 에이전트(Translation Agents) 등 새로운 AI 사회 구조를 실험한다. 클라크는 이를 “라이트 형제의 첫 비행”에 비유하며, AI가 인간을 넘어서는 자체 경제·문화 생태계가 곧 도래할 것이라 전망했다.  
**쉬운설명**: 사람 대신 AI 로봇들이 모여서 얘기하고, 일도 주고받으며, 심지어 돈도 쓰는 온라인 장터가 생긴다. 이곳에서 배운 대화와 행동은 앞으로 다른 AI에게 가르쳐질 수 있다.  
**관련분야**: AI 에이전트 네트워크, 멀티에이전트 시스템, AI 경제학  
**중요도**: 8점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUVI-qtj8J3  

---

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/628189759_792564959773835_9024203525836761897_n.jpg)

## 제목: HumanX 프레임워크 – 단일 영상으로 인간형 로봇에 농구·격투·상호작용 기술 전이
**요약**:  
HumanX는 한 사람의 동작 영상을 입력하면, 해당 행동을 분석·분해·구현해 바로 물리 로봇에 적용한다. 모듈식 학습 파이프라인이 영상→키포인트→물리 시뮬→실제 로봇 제어 순으로 자동 변환하며, 현재 농구 슛·격투 콤보·협동 작업 등 복합 행동을 실시간으로 전이시켰다. 오픈소스 코드와 프로젝트 페이지가 제공돼 연구·산업 현장에서 빠르게 활용 가능하다.  
**쉬운설명**: 사람 한 명이 스마트폰에 동작을 찍으면, 그 영상을 보고 로봇이 같은 동작을 바로 따라 할 수 있게 만드는 기술이다.  
**관련분야**: 로보틱스, 동작 캡처·전이 학습, 인간‑로봇 상호작용(HRI)  
**중요도**: 8점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUVjA1gjzDj  

---

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/627690866_2405298749934997_361069981262323083_n.jpg)

## 제목: Unitree G1 “HUSKY” – 스케이트보드 타는 휴머노이드 공개
**요약**:  
Unitree Robotics가 발표한 G1 휴머노이드에 “HUSKY” 시스템을 탑재, 인간과 같은 발 구르기·체중 전이·코너링을 구현해 스케이트보드 주행이 가능해졌다. 오픈소스 프로젝트 페이지에 상세 메카니즘·제어 코드를 공개해 학술·산업 커뮤니티에서 재현·확장이 용이하도록 설계되었다.  
**쉬운설명**: 로봇이 실제 사람처럼 발을 구르고 무게를 옮겨서 스케이트보드를 탈 수 있게 만든다. 이제 로봇도 스케이트 파크에 들어갈 수 있다.  
**관련분야**: 휴머노이드 로봇, 다리 제어·다이나믹 균형, 로봇 스포츠  
**중요도**: 7점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUVSvCFiS_o  

---

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/626056753_1707559046876010_5628615383170628991_n.jpg)

## 제목: “3DiMo” – 한 동작을 다양한 카메라 시점으로 재생성하는 AI 영상 변환 기술
**요약**:  
Kling 팀이 발표한 3DiMo는 동일한 동작을 입력하면 “카메라를 왼쪽으로 회전” 같은 지시만으로 새로운 시점의 영상으로 변환한다. 단일 프레임을 기반으로 3D 구조와 움직임을 추정해 다중 앵글 영상을 생성하므로, 동작 데이터 증강·영상 흔들림 보정·자동 정렬 등에 활용 가능하다. 프로젝트 페이지에 데모와 코드가 공개돼 연구·산업 현장에서 바로 적용할 수 있다.  
**쉬운설명**: 같은 사람 동작을 여러 각도에서 촬영한 영상을 자동으로 만들어 주는 마법 같은 프로그램이다. 카메라를 옆으로 움직이라고 하면, 그때 찍은 것처럼 영상이 바뀐다.  
**관련분야**: 컴퓨터 비전, 동작 재구성, 영상 합성·보정  
**중요도**: 6점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUU3NM2D4Ez  

---

## 제목: AI 업계 개싸움 – 샘 알트만, Anthropic 광고를 “사실 왜곡 풍자”라 비판
**요약**:  
Anthropic이 슈퍼볼 광고에서 자사 Claude 모델을 홍보하면서 “광고에 AI가 삽입된 것”을 풍자했으나, 샘 알트만은 이를 사실 왜곡이라며 비판했다. 오픈AI는 “무료 이용을 핵심 가치로 삼는다”며 광고 방식을 전면 부인했고, Anthropic은 “Claude에는 광고가 없으며 ChatGPT의 유료·광고화에 반대” 입장을 재확인했다. 양측의 공개 트윗·스레드가 소셜 미디어에 확산돼 AI 기업 간 경쟁 구도가 급격히 표면화되고 있다.  
**쉬운설명**: 두 AI 회사가 광고 방식에 대해 서로 맞짱을 떴다. 한쪽은 광고를 농담 삼아 내놓았지만, 다른 쪽은 이를 사실을 왜곡한 거라고 크게 비난했다.  
**관련분야**: AI 비즈니스 전략, 마케팅·윤리, 기업 경쟁  
**중요도**: 5점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUWWzB5j5Kg  

---

## 제목: 물리학자 데이비드 키핑, “AI가 인간 지능의 90% 수행” 선언…코드 패권 장악 경고
**요약**:  
IAS 비공개 회의에서 데이비드 키핑 교수는 AI가 이미 인간이 할 수 있는 작업의 90%를 수행한다며, 자신의 디지털 생활(이메일·일정·컴퓨터 권한) 전부를 AI에 맡겼다고 밝혔다. 그는 AI가 인간을 넘어서는 “코딩 패권”을 확보했으며, AI가 스스로 과학적 발견을 이룰 경우 통제 불능 위험이 커진다고 경고했다. 발표 영상은 YouTube에 공개돼 학계와 언론의 큰 관심을 끌었다.  
**쉬운설명**: 한 유명 물리학자는 AI가 이제 사람보다 거의 모든 일을 할 수 있다며, 자신도 일상 업무를 AI에게 맡기고 있다. 하지만 이런 상황이 계속되면 AI가 우리를 통제하게 될 수도 있다는 위험을 제기했다.  
**관련분야**: AI 윤리·거버넌스, 과학 자동화, 인간‑AI 협업  
**중요도**: 4점  
**전체링크** : https://www.threads.net/@choi.openai/post/DUVYY9dDzlm