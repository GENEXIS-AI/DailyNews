![Image](https://scontent-iad3-2.cdninstagram.com/v/t51.71878-15/590418817_2505855283142793_7140002023330500374_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=111&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWgel3VybGdlbi5DMyJ9&_nc_ohc=kWCziydAW8wQ7kNvwGIuVcE&_nc_oc=Adn7helu02sMaoIMdA0Me_Gct1FkNAlAaOtr11iXQBQpopjAUDD9-504rlxXIORavDk&_nc_zt=23&_nc_ht=scontent-iad3-2.cdninstagram.com)

## 제목:
Meta AI가 “Saber”를 공개했습니다.

**요약**:  
Meta AI가 영상‑텍스트 쌍만으로 학습한 텍스트‑투‑비디오 모델 “Saber”를 발표했습니다. 마스크‑증강(Masked training & mask augmentation) 전략을 도입해 복사‑붙여넣기 아티팩트를 억제하고, 레퍼런스 이미지 속 캐릭터를 원본 정체성을 유지한 채 움직이는 영상으로 변환합니다. OpenS2V‑Eval 벤치마크에서 기존 전용 데이터셋으로 훈련한 경쟁 모델들을 앞서며 SOTA를 달성했습니다.

**쉬운설명**:  
‘Saber’는 사진 한 장만 주면 그 인물이나 사물을 실제 움직이는 동영상으로 바꿔 주는 AI입니다. 비싼 특수 데이터 없이 “영상‑텍스트” 쌍만으로 배워서, 사진을 그대로 복제하는 게 아니라 자연스럽게 움직이게 합니다. 마스크라는 ‘가리기’ 방법을 써서 흔히 보이는 흔적(아티팩트)을 없앴습니다.

**관련분야**: 생성형 AI·텍스트‑투‑비디오·멀티모달 학습  

**중요도**: 9점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSFy6UYkhTc  

---  

![Image](https://scontent-iad3-2.cdninstagram.com/v/t51.71878-15/586676316_1356904142137524_2127448763868835620_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=100&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=MMs0H8CyRcIQ7kNvwF4j11A&_nc_oc=AdlIj5Z-ZuHUSUzGXhUn1gqYZC5FwYjkmFN16tjBsov0UBB4e7iMtzluD49FiO61lhI&_nc_zt=23&_nc_ht=scontent-iad3-2.cdninstagram.com)

## 제목:
Snap이 놀라운 기술을 공개했습니다.

**요약**:  
Snap Research가 실시간 1인칭(egocentric) 영상 편집 프레임워크 “EgoEdit”를 발표했습니다. 사용자가 보는 화면에 보이는 물체를 즉시 다른 물체로 교체하거나 제거할 수 있으며, 손‑물체 상호작용이 깨지지 않는 것이 핵심입니다. H100 GPU 1대 기준 855 ms 지연, 38.1 FPS를 달성했으며, “Wan 2.1” 모델을 증류해 제공하고 데이터·벤치마크도 공개했습니다.

**쉬운설명**:  
‘EgoEdit’는 눈앞에 보이는 장면을 마치 마법처럼 바꿔 주는 기술입니다. 예를 들어 손에 든 컵을 바로 다른 물건으로 바꾸고, 손 움직임은 그대로 유지됩니다. 딱 한 대의 고성능 GPU만 있으면 1초 안에 바뀐 영상을 볼 수 있어, AR 안경에 바로 적용하기 좋은 수준입니다.

**관련분야**: 증강현실·실시간 영상 편집·컴퓨터 비전  

**중요도**: 9점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSJ_KEj8nQ  

---  

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/597915044_861217356414111_3570308750399908890_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=105&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=5orO6YPEWckQ7kNvwF_m1SM&_nc_oc=AdkEgivyYbH1gPJA0KEVsozDYpXpF1PAnYnHxkyfW-lJY2hDzxgOJNRBTUDBW_1jdDU&_nc_zt=23&_nc_ht=scontent-iad3-2.cdninstagram.com)

## 제목:
일론 머스크는 “비감독 FSD(자율주행) 문제는 현시점에서 거의 해결되었다”고 발표했습니다.

**요약**:  
테슬라가 3주 내 텍사스 오스틴에서 안전 요원 없는 로보택시 서비스를 시작한다고 선언했습니다. 차세대 모델은 기존 대비 규모가 한 차원 크게 확대되고, 고도화된 추론·강화 학습이 추가돼 논리적 판단까지 가능해질 예정입니다. 또한 자체 반도체 팹 설립까지 검토 중이라고 밝혔습니다.

**쉬운설명**:  
테슬라가 사람 없는 택시를 곧 도심에서 시험 운행합니다. 새 차량은 더 큰 컴퓨터와 똑똑한 학습 알고리즘을 탑재해 단순 운전뿐 아니라 복잡한 상황 판단도 할 수 있게 됩니다. 앞으로는 직접 칩을 만들 필요가 있을 정도로 AI 하드웨어에 집중한다는 뜻입니다.

**관련분야**: 자율주행·모빌리티·AI‑칩  

**중요도**: 8점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSFDE41DxAU  

---  

## 제목:
앤트로픽이 LLM의 뇌 구조를 물리적으로 뜯어고치는 기술 “SGTM(Selective Gradient Masking)”을 공개했습니다.

**요약**:  
SGTM은 모델 파라미터를 ‘기억용’과 ‘망각용’ 두 영역으로 분리하고, 위험하거나 비윤리적인 지식이 포함된 파라미터만 선택적으로 업데이트·삭제하도록 설계했습니다. 위험 데이터가 라벨링되지 않아도 자동 흡수 효과가 있어, 사후 망각보다 7배 이상 복구가 어려운 안전성을 제공합니다. 연산 비용은 기존 대비 5 % 수준에 불과합니다.

**쉬운설명**:  
‘SGTM’은 인공지능의 기억 중 위험한 부분만 골라서 지우는 수술 같은 기술입니다. 위험한 정보가 들어오면 그 부분만 따로 표시해두고, 나중에 필요하면 완전히 없앨 수 있습니다. 일반적인 지식은 그대로 유지하면서 위험한 내용만 안전하게 차단합니다.

**관련분야**: AI 안전·모델 정제·프롬프트 엔지니어링  

**중요도**: 8점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSE4eSlCUgf  

---  

## 제목:
Nous Research가 오픈소스 모델 “Nomos 1 30B”를 공개했습니다.

**요약**:  
Nomos 1 30B는 Putnam 수학 경시대회(120점 만점)에서 87점을 획득해 2024년 전체 참가자 중 2위에 해당합니다. 동일 조건에서 Qwen‑3가 24점에 그친 것과 대비해, 모델 규모보다 데이터 품질·사후 학습이 성능에 결정적임을 입증했습니다. 모델과 코드 모두 Hugging Face에 공개되었습니다.

**쉬운설명**:  
‘Nomos 1 30B’는 수학 문제를 푸는 AI인데, 미국 최고 수준의 대학생 수학 경시대회에서 거의 최상위 성적을 거뒀습니다. 큰 모델보다 좋은 데이터와 추가 학습이 얼마나 중요한지를 보여준 사례입니다.

**관련분야**: 수학 AI·오픈소스 LLM·지식 추론  

**중요도**: 7점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSFQjv5AIPw  

---  

![Image](https://scontent-iad3-2.cdninstagram.com/v/t51.71878-15/598550158_17933872326112832_1294244838924777061_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=106&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=JituUZFqdDMQ7kNvwFqhc7W&_nc_oc=Admi7llg6uTXi7PxiWxku1m8hYfwPkIi32a3jQ8VCbt7XJgY9T3vqVVZyjD-sdAkMaQ&_nc_zt=23&_nc_ht=scontent-iad3-2.cdninstagram.com)

## 제목:
허깅 페이스가 “Hugging Face Skills”를 공개했습니다.

**요약**:  
Hugging Face Skills를 Claude Code와 연동하면 자연어 명령만으로 데이터셋 검증, 클라우드 GPU 대여, 학습 모니터링, 모델 배포까지 전 과정을 에이전트가 자동 수행합니다. SFT·DPO 같은 실무 수준 미세조정도 지원하며, 소형 모델(≤7B) 기준 비용은 약 0.3 달러 수준으로 크게 낮춰졌습니다. 대형 모델은 아직 제한적이며 유료 계정이 필요합니다.

**쉬운설명**:  
‘Hugging Face Skills’는 “말만 하면 AI 모델을 학습시켜 주세요” 라는 마법 같은 도구입니다. 데이터를 준비하고, GPU를 빌리고, 학습 과정을 체크하고, 완성된 모델을 내보내는 모든 과정을 한 줄 명령으로 자동화합니다. 작은 모델은 아주 저렴하게 훈련할 수 있습니다.

**관련분야**: LLM 파인튜닝·자동화·머신러닝 인프라  

**중요도**: 7점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSEj5cZCKyi  

---  

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/592474379_871082742040218_7403215537194647149_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=110&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=NSR26I6_bBsQ7kNvwFKIWbU&_nc_oc=AdkDDFiqdm3wDW3YXjXQr3x3v9Uko80mLJ-j2bx3xlN2bjcuJEl2-mSSmGpXawF0j6c&_nc_zt=23&_nc_ht=scontent-iad3-1.cdninstagram.com)

## 제목:
Hedra가 “Audio Tags” 기능을 출시했습니다.

**요약**:  
Hedra의 Audio Tags는 오디오 파일에 [angry], [whisper], [laughing] 등 감정 태그를 직접 부착할 수 있게 합니다. Kling Avatars·Character‑3 모델과 결합하면 감정 라인에 따라 입모양·표정의 미세한 떨림까지 자연스럽게 생성됩니다. 기존 AI 아바타의 로봇‑같은 어색함을 크게 완화할 핵심 기술로 기대됩니다.

**쉬운설명**:  
‘Audio Tags’는 녹음 파일에 “화난”, “속삭이는” 같은 감정을 표시해 주는 라벨을 붙이는 기능입니다. 이렇게 라벨을 달면 AI 캐릭터가 그 감정에 맞는 입모양·표정을 실시간으로 만들어 줍니다. 이제 음성만 들려도 자연스러운 감정 표현이 가능합니다.

**관련분야**: 감정 AI·음성 합성·디지털 아바타  

**중요도**: 5점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSFgneUCGYo  

---  

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/588976461_892947983685306_3404999931066696423_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=107&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=ByZPxkbMO8sQ7kNvwH3fSxE&_nc_oc=Adk9-uPlPCpteGaIMJr-4kAAvhPg3y6arBeebdetywF99pQYwMUdZS8evXi8qvCI07M&_nc_zt=23&_nc_ht=scontent-iad3-1.cdninstagram.com)

## 제목:
‘바이브 코딩 IDE’를 표방하는 Orchids가 공개되었습니다.

**요약**:  
Orchids는 화면·음성을 인식해 개발자를 돕는 “바이브 코딩 IDE”를 출시했습니다. Supabase·Stripe가 내장돼 있어 백엔드·결제 기능을 즉시 사용할 수 있으며, 에이전트·브라우저·IDE가 하나로 통합돼 설정 없이 바로 서비스 제작이 가능합니다. App Bench 벤치마크에서 1위를 차지해 실무 적용 가능성을 입증했습니다.

**쉬운설명**:  
‘Orchids’는 “코드가 보이면 자동으로 이해하고, 말하면 바로 구현한다”는 AI 기반 개발 환경입니다. 화면을 보면서 말만 하면 코드를 자동 생성하고, 필요한 데이터베이스와 결제 시스템까지 자동으로 연결해 줍니다. 별도 설정 없이 바로 웹·앱을 만들 수 있습니다.

**관련분야**: AI 코딩 어시스턴트·통합 개발 환경·노코드/로우코드  

**중요도**: 5점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSEZAujD3Z2  

---  

![Image](https://scontent-iad3-1.cdninstagram.com/v/t51.71878-15/588464488_2240306159785233_2320228554803790871_n.jpg?stp=dst-jpg_e35_tt6&_nc_cat=102&ccb=7-5&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiRkVFRC5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=9yd35TkntpAQ7kNvwHlVgiL&_nc_oc=AdkwOX5-tYY0Hp2rxqmrT9Df1nHWovXVrANfjmHEdSM5ug5OaaV0G5_9DrByvl_EiIE&_nc_zt=23&_nc_ht=scontent-iad3-1.cdninstagram.com)

## 제목:
데이터 라벨링이 정말 쉬워지고 있습니다.

**요약**:  
Roboflow가 “Rapid”를 출시했습니다. 사용자는 영상·이미지를 업로드하고 찾고 싶은 객체를 텍스트 프롬프트로 입력하면, SAM‑3가 자동 라벨링을 수행하고 지식 증류를 통해 90초 안에 객체 탐지 API를 생성합니다. 생성된 코드는 파이썬·자바스크립트로 바로 내보낼 수 있어 서비스 적용이 즉시 가능해집니다.

**쉬운설명**:  
‘Rapid’는 사진이나 동영상에 “고양이” 같은 단어만 입력하면 AI가 자동으로 고양이 영역을 박스 처리해 줍니다. 별도 수작업 없이 몇 초 만에 라벨링된 객체 탐지 모델을 만들 수 있어, 개발자가 직접 그림을 그릴 필요가 없습니다.

**관련분야**: 컴퓨터 비전·데이터 라벨링·자동화 AI  

**중요도**: 5점  

**전체링크** : https://www.threads.net/@choi.openai/post/DSEPpFHD_5c  