![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04467.png)
## 제목: 
VisionZip: Longer is Better but Not Necessary in Vision Language Models

**요약**: 최근 비전-언어 모델(Vision Language Models, VLM)의 성능은 시각적 토큰의 길이를 증가시킴으로써 향상되어 왔으나, 이는 계산 비용을 크게 증가시켰습니다. VisionZip은 간단하면서도 효과적인 방법으로, 언어 모델에 입력되는 유익한 토큰 세트를 선택하여 시각적 토큰의 중복성을 줄이고 효율성을 높이면서도 성능을 유지합니다. 이 방법은 이미지와 비디오 이해 작업에 널리 적용될 수 있으며, 특히 현실 세계의 다회 대화에서 우수한 성능을 발휘합니다. 실험 결과, VisionZip은 성능을 5% 이상 개선하였고, 추론 속도를 크게 향상시켰습니다.

**쉬운설명**: VisionZip은 이미지와 언어를 동시에 이해하는 모델의 성능을 더 좋게 하면서도 필요한 계산량을 줄이는 방법을 제공합니다. 이것은 읽어야 할 시각 정보의 양을 줄여서 동일한 성능을 유지하는 동시에 더 빠르게 작동하도록 합니다.

**관련분야**: 비전-언어 모델, 이미지 및 비디오 처리

**추천수**: 59

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2412.04467)