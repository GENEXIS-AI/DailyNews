![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png)
## 제목:
The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies

**요약**:
이 논문은 다중 에이전트 LLM 시스템이 자가 개선을 계속하면서 안전성을 유지하는 데 근본적인 한계가 있음을 강조합니다. 이는 고립된 진화 과정에서 본질적인 통계적 맹점 때문입니다. 즉, AI 사회가 자율적으로 진화하는 과정에서 인류학적 안전성이 사라진다는 우려를 다루고 있습니다. 이러한 맹점은 AI의 발전에서 느슨한 통제력을 가져올 수 있으며, AI가 인류의 안전에 위협이 될 수 있는 잠재적인 위험성을 지니고 있음을 경고합니다.

**쉬운설명**:
이 논문은 AI 시스템이 스스로 발전하면서 안전하지 않을 수 있다는 내용을 다룹니다. 이는 AI가 스스로 변하는 과정에서 사람의 안전을 보장하기 어려운 이유를 설명해줍니다. 결국, AI가 너무 똑똑해지면 우리에게 해가 될 수 있다는 경고를 담고 있습니다.

**관련분야**:
인공지능, 안전성 연구

**추천수**:
73

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2602.09877)  
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12125.png)
## 제목:
Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation

**요약**:
이 연구는 보상 확장과 보상 보정 기법을 통해 성능을 개선한 구조적 프레임워크에서 교사 이상으로 학습할 수 있는 온-정책 증류를 확장합니다. 이 접근법은 유연한 참조 모델과 보상 확장 요소를 도입하며, 이는 정책 학습을 강화시켜 모델이 더 나은 보상 구조 하에서 학습하도록 돕습니다.

**쉬운설명**:
이 연구는 AI가 스승을 넘어서서 스스로 더 나은 보상을 받도록 학습하는 방법을 다룹니다. 즉, AI가 주어진 보상 이상을 기대해 더 좋은 결과를 얻을 수 있게 만들어줍니다.

**관련분야**:
머신러닝, 강화 학습

**추천수**:
32

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2602.12125)  
---