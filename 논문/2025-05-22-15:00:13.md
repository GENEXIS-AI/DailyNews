![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15277.png)
## 제목:
Web-Shepherd: Advancing PRMs for Reinforcing Web Agents

**요약**: 이 논문에서는 웹 탐색 에이전트의 훈련과 테스트 시 사용 가능한 최초의 PRM(Process Reward Model)인 Web-Shepherd를 소개합니다. Web-Shepherd는 웹 탐색 경로를 스텝 수준에서 평가할 수 있으며, 이를 위해 크게 두 가지 새로운 도구를 도입했습니다: 40K 스텝 수준의 선호 쌍 및 다양한 도메인과 난이도를 다루는 WebPRM Collection 데이터셋, 그리고 PRM 평가를 위한 최초의 메타 평가 벤치마크인 WebRewardBench입니다. 실험 결과, Web-Shepherd는 기존 모델들에 비해 성능과 비용 효율성이 크게 개선되었음을 보여주었습니다.

**쉬운설명**: 웹 페이지를 탐색하는 프로그램이 길고 복잡한 작업을 수행할 때, 이를 잘 수행했는지를 평가하는 프로그램(Web-Shepherd)을 새로 만들었어요. 이 프로그램은 더 정확하고 저렴하게 이 일을 할 수 있게 해 줍니다. 

**관련분야**: 인공지능, 데이터 분석, 웹 탐색 에이전트, 기계학습

**추천수**: 58

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2505.15277)
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14302.png)
## 제목:
Scaling Law for Quantization-Aware Training

**요약**: 이 논문은 QAT(Quantization-Aware Training)의 스케일링 법칙을 제시하여 모델 크기, 훈련 데이터 양, 그리고 양자화 그룹 크기에 따라 양자화 오류가 어떻게 변화하는지를 설명합니다. 연구 결과, Weight와 Activation의 양자화 오류가 각기 다른 경향을 보이며, 특히 Weight 양자화 오류가 훈련 데이터 증가 시 더 민감하게 반응합니다. 또한, FC2 계층에서 발생하는 Activation 오류가 주요 병목 현상임을 찾아 이를 해결하기 위해 중요하고 다양한 양자화를 적용하였습니다.

**쉬운설명**: 컴퓨터가 크고 복잡한 모델을 더 작게 만들어 사용하되, 성능을 유지하기 위해 양자화 기술을 사용해요. 이 연구는 그러한 기술을 사용할 때 발생하는 오류와 그 해결 방법을 제시합니다.

**관련분야**: 양자화 훈련, 인공지능 모델 최적화, 기계학습

**추천수**: 38

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2505.14302)
---

이런 방식으로 나머지 논문에 대한 요약과 설명을 원하시나요? 필요한 논문에 대한 질문이 있으시면 말씀해 주세요!