![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08313.png)

## 제목:
MiniMax-01: Scaling Foundation Models with Lightning Attention

**요약**:
MiniMax-01 시리즈는 MiniMax-Text-01과 MiniMax-VL-01을 포함하며, 장문 컨텍스트를 처리하는 능력이 우수한 모델입니다. 핵심은 lightning attention과 효율적인 확장에 있으며, Mixture of Experts(MoE)와 통합하여 4560억 개의 매개변수로 구성된 모델을 생성했습니다. MiniMax-Text-01은 훈련 중 최대 100만 토큰의 컨텍스트 윈도우를 가지고 추론 시 400만 토큰까지 처리할 수 있습니다. 실험 결과, 최신 모델들과 견주어 성능이 대등하면서도 20~32배 긴 컨텍스트 윈도우를 제공합니다.

**쉬운설명**:
MiniMax-01은 긴 문장을 읽고 이해하는 데 강점을 가진 인공지능 모델입니다. 이 모델은 매우 많은 데이터를 동시에 처리할 수 있어 긴 문서의 내용을 빠르고 정확하게 파악할 수 있습니다.

**관련분야**:
AI 모델 확장, NLP, 장문 컨텍스트 처리

**추천수**:
130

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2501.08313)