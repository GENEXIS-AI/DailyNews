### 1. **Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space**

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13835.png)

**요약**:
이 논문은 대규모 데이터에서 정보 이득을 극대화하여 고품질 및 다양한 데이터 하위 집합을 자동으로 선택하는 **MIG(Maximizing Information Gain)** 방법을 제안합니다. 기존 방법은 인스턴스 품질을 우선시하고 다양성을 유지하기 위해 휴리스틱 규칙을 사용하지만, 이는 복잡한 지시의 의미 공간에서의 의도를 정확히 포착하지 못합니다. MIG는 레이블 그래프를 구축하고 이 그래프 내의 정보 분포에 기초하여 다양성을 측정함으로써 이러한 문제를 해결합니다. 다양한 데이터셋과 기본 모델에 대한 실험들은 MIG가 최첨단 방법들보다 일관되게 우수함을 보여줍니다.

**쉬운설명**:
데이터의 질과 다양성은 컴퓨터가 명령을 잘 처리하도록 돕는 중요한 요소입니다. 이 논문에서는 정보의 양을 늘리는 방법으로, 좋은 데이터를 자동으로 선택할 수 있는 방법을 소개합니다. 이렇게 하면 더 적은 데이터로도 좋은 결과를 얻을 수 있습니다.

**관련분야**:
데이터 분석, 자연어 처리, 머신러닝

**추천수**:
22

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2504.13835)

---

### 2. **Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?**

![Image](https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg)

**요약**:
이 연구는 **강화학습(Reinforcement Learning, RL)**이 대규모 언어 모델(LLMs)의 추론 능력을 향상시키는지에 대한 기존의 믿음을 재평가합니다. 큰 'k' 값을 가진 pass@k 메트릭을 측정하여 RL이 근본적으로 새로운 추론 패턴을 만들어내지 않는다는 것을 발견했습니다. 대부분의 경우 RL로 훈련된 모델의 추론 경로는 이미 기본 모델의 샘플링 분포에 포함되어 있습니다. 이는 RL 학습이 모델의 출력 분포를 보상 가능성이 높은 경로로 편향시킴으로써 정확한 응답을 더 효율적으로 샘플링할 수 있지만 추론 능력의 경계를 좁힌다는 것을 의미합니다.

**쉬운설명**:
많은 사람들이 강화학습이 컴퓨터의 생각하는 힘을 확실히 높여준다고 믿습니다. 하지만, 이 연구는 꼭 그렇지 않다는 것을 설명합니다. 강화학습으로 컴퓨터가 더 잘할 수 있을 때도 있지만, 기본적인 컴퓨터도 충분히 좋은 결과를 만들어낼 수 있다는 점을 강조합니다.

**관련분야**:
강화학습, 인공지능, 머신러닝

**추천수**:
17

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2504.13837)

---