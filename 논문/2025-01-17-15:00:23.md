![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09732.png)

## 제목: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps

**요약**: 이 논문은 생성 모델, 특히 디퓨전 모델의 추론 단계에서 스케일링을 어떻게 향상시킬 수 있는지를 탐구합니다. 디퓨전 모델은 본질적으로 추론 시 덴와이징(Denoising) 단계의 수를 조절할 수 있지만, 성능 향상은 보통 수십 단계 후에는 둔화됩니다. 저자는 디퓨전 샘플링 과정에서 더 나은 노이즈를 식별하는 검색 문제를 고려하여, 런타임 컴퓨팅 증가를 통해 생성 성능이 향상될 수 있음을 보여줍니다.

**쉬운 설명**: 디퓨전 모델이 이미지를 더 잘 만들 수 있도록 컴퓨터가 더 많은 생각을 하게 만드는 방법을 연구한 논문입니다. 더 나은 노이즈를 찾는 데 집중하여, 컴퓨터가 더 좋은 결과를 낼 수 있도록 했습니다.

**관련 분야**: 생성 모델, AI 추론 최적화

**추천수**: 6

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2501.09732)

---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08617.png)

## 제목: RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation

**요약**: 이 논문에서는 RLHF(Reinforcement Learning from Human Feedback)의 잘못된 정렬을 완화하기 위한 새로운 방법으로 RLHS(Reinforcement Learning from Hindsight Simulation)를 제안합니다. 기존 RLHF 방식은 즉각적인 피드백에 의존하여 사용자 유틸리티에 미치는 장기적인 영향을 반영하지 못하는 문제를 지적합니다. 대신, 역량 평가를 통해 장기적 결과를 시뮬레이션하고 이로부터 학습하는 전략을 제시합니다. RLHS는 PPO 및 DPO 방법에 적용하여 실험적으로 잘못된 정렬을 상당히 줄이고 사용자의 목표 달성 및 만족도를 향상시켰음을 입증했습니다.

**쉬운 설명**: 이 논문은 AI 시스템이 사람의 피드백을 받을 때, 단기적인 결과보다는 행동이 앞으로 미칠 영향을 더 잘 평가하도록 돕는 방법을 탐구한 것입니다. 이런 방법으로 AI가 더 신뢰할 수 있는 행동을 하도록 하는 방법을 제안했습니다.

**관련 분야**: 인공지능 정렬, 강화 학습

**추천수**: 4

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2501.08617)

---