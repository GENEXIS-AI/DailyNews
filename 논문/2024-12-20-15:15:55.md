![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15115.png)
## 제목: 
Qwen2.5 Technical Report

**요약**:
이번 보고서는 Qwen2.5라는 대규모 언어 모델(LLM) 시리즈를 소개합니다. Qwen2.5는 다양한 요구를 충족하기 위해 이전 버전보다 훨씬 개선된 모델입니다. 사전 학습 단계에서는 고품질 데이터셋을 7조 토큰에서 18조 토큰으로 확장하였으며, 이는 상식, 전문 지식, 그리고 추론 능력을 강화합니다. 사후 학습 단계에서는 백만 개 이상의 샘플을 사용하여 세심한 감독 미세 조정과 다단계 강화 학습을 적용하여 기호와 구조적 데이터 생성, 지시사항 따라하기 등에 탁월한 성능을 보입니다. Qwen2.5는 다양한 크기로 제공되며, 오픈 웨이트 모델과 믹스처 오브 익스퍼트(MoE) 변형을 포함합니다. 오픈 웨이트 대표 모델인 Qwen2.5-72B-Instruct는 다른 여러 모델들을 뛰어넘는 성과를 보이며, 대기업 클라우드에서 사용할 수 있는 Qwen2.5-Turbo 및 Qwen2.5-Plus는 비용 효율성을 제공합니다. 또한 Qwen2.5 모델은 Qwen2.5-Math, Qwen2.5-Coder, QwQ와 같은 특화된 모델 훈련의 기초가 되고 있습니다.

**쉬운설명**:
이 보고서는 최신 기술을 사용하여 다양한 일을 잘 해낼 수 있는 새로운 언어 모델인 Qwen2.5를 소개합니다. 예전보다 더 많은 정보를 학습해서 똑똑해졌고, 특정 지시를 잘 따르는 능력도 좋아졌습니다. 다양한 규모로 제공되며, 저장소에서도 사용할 수 있는 특별한 버전도 있습니다.

**관련분야**: 인공지능, 언어 모델, 자연어 처리

**추천수**: 130

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2412.15115)

---