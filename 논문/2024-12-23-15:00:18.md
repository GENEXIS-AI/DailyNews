![Image](https://avatars.githubusercontent.com/u/4d9f9a546aa8c63e277161ea700075c4?v=4)
## 제목:
**Parallelized Autoregressive Visual Generation**

**요약**:
이 논문에서는 시각적 생성(imagery generation)을 위해 자주 사용되는 오토리그레시브 모형(autoregressive model)의 느린 추론 속도를 개선하기 위한 간단하고 효과적인 접근법을 제안하고 있습니다. 오토리그레시브 모형은 시퀀셜한 토큰 예측 때문에 시간 소요가 큽니다. 새로운 방법은 시각적 토큰의 의존성에 기반하여, 의존성이 적은 토큰은 병렬로 생성하고, 강한 인접 토큰 의존성으로 인해 순차적으로 생성해야 하는 경우를 구분하여 최적의 생성 전략을 수립합니다. 이를 통해 인프라의 변형 없이 기존 오토리그레시브 모형에 적용 가능하며, ImageNet과 UCF-101 데이터셋 실험에서 품질을 유지하며 3.6배 빠른 생성 속도를 보여주며, 최소한의 품질 저하로 최대 9.5배의 성능 향상을 제시합니다.

**쉬운설명**:
이 논문은 이미지나 비디오와 같은 시각적 데이터를 생성하는 방법 중에서 더 빠르게 실행할 수 있는 효과적인 방법을 제안하고 있습니다. 기존 방법은 하나하나 순서대로 생성하느라 시간이 오래 걸리지만, 이 논문은 서로 별로 영향을 주지 않는 부분은 동시에 만들 수 있다는 아이디어를 바탕으로 개선한 것입니다.

**관련분야**:
시각적 데이터 생성, 오토리그레시브 모델, 효율적인 데이터 생성

**추천수**:
12

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2412.15119)
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13649.png)
## 제목:
**SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation**

**요약**:
이 논문에서는 긴 문맥 생성(long-context generation)에서 발생하는 주요 문제를 해결하기 위해 키-값(Key-Value) 캐시의 최적화를 위한 새로운 프레임워크인 SCOPE를 소개합니다. 이 방법은 프리필(prefill) 단계와 디코딩 단계에서 캐시 최적화를 각각 수행하여 기존 방법에서 흔히 간과되던 디코딩 최적화를 강조합니다. 특별한 방법론을 통해 메모리 사용량을 최적화하고, 초기단계의 필수 정보를 유지하며, 필요한 정보만을 효과적으로 선택하는 방식을 사용합니다. LongGenBench 실험에서 그 효과성과 일반화 가능성을 보여주었습니다.

**쉬운설명**:
장시간 내용을 만들어낼 때 중요한 정보를 저장하고 찾는 방식이 중요한데, 이 논문은 그 방식에서 최적화를 시도하고 있습니다. 특히 잘 쓰이지 않는 정보는 덜 저장하고, 중요한 정보는 잘 관리하여, 길게 내용을 만드는 일을 효율적으로 수행할 수 있도록 돕습니다.

**관련분야**:
대형 언어 모델, 데이터 압축과 최적화, 장시간 컨텍스트 생성

**추천수**:
8

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2412.13649)
---