![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13161.png)

## 제목:
CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training

**요약**:
이 논문에서는 웹에서 수집된 사전 학습 데이터셋의 최적화된 데이터 조합을 찾기 위한 자동화된 프레임워크 'CLIMB'을 제안합니다. CLIMB는 대규모 데이터셋을 의미론적 공간에서 임베딩하고 클러스터링하여 최적의 데이터 조합을 반복적으로 탐색하는 방식으로 작동합니다. 400B 토큰의 데이터로 1B 모델을 학습한 결과, Llama-3.2-1B보다 2% 개선된 성능을 보였으며, 특정 도메인에 최적화하면 무작위 샘플링보다 5% 향상된 성과를 보였습니다. 

**쉬운설명**:
CLIMB는 언어 모델을 훈련하기 위해 데이터를 더 스마트하게 선택하는 방법입니다. 큰 데이터를 작은 부분으로 나누어 각 부분이 얼마나 도움이 되는지 평가하고 가장 좋은 조합을 찾습니다. 이렇게 하면 모델이 더 똑똑해지고 다양한 내용에 대응할 수 있습니다.

**관련분야**:
자연어 처리, 기계 학습, 데이터 혼합

**추천수**:
71

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2504.13161)
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13146.png)

## 제목:
Antidistillation Sampling

**요약**:
이 논문은 모델 소유자가 특정 상황에서 원하지 않는 모델 증류를 제한하는 샘플링 전략으로서 'Antidistillation Sampling'을 제안합니다. 이는 모델이 예측하는 다음 토큰의 확률 분포를 수정함으로써 증류에 덜 효과적인 추론 이력을 만들어내면서도 모델의 실제 응용 가능성을 유지합니다.

**쉬운설명**:
Antidistillation Sampling은 모델이 잘못 배워서 오히려 도움 되는 결과를 내지 못하게 하는 방법입니다. 특정 상황에서는 정말 필요한 정보만을 남겨 모델의 실력을 유지하는 방법이죠.

**관련분야**:
기계 학습, 데이터 증류, 샘플링 전략

**추천수**:
53

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2504.13146)
---

---

어떤 요약을 더 제공해드릴까요?