![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.14572.png)

## 제목:
**CURLoRA: Stable LLM Continual Fine-Tuning과 Catastrophic Forgetting 완화**

## 요약:
이 논문에서는 **CURLoRA**라는 새로운 접근 방식을 소개합니다. 이는 **CUR 행렬 분해**를 **LoRA (Low-Rank Adaptation)** 문맥에서 활용하여, **대형 언어 모델(Large Language Models, LLMs)**의 지속적인 학습 중에서 발생하는 **Catastrophic Forgetting**을 완화하고 훈련 가능한 파라미터 수를 줄이는 방법입니다. CUR 분해 과정에서 열과 행을 선택할 때 **반전된 확률**을 사용하여 **암묵적 정규화(Implicit Regularization)**를 수행하고, U 행렬을 0으로 초기화한 후에만 세부 조정하는 독특한 수정 방식을 제안합니다. 여러 데이터 세트를 통해 표준 LoRA보다 Catastrophic Forgetting을 완화하는 데 있어 뛰어난 성능을 보여주며, 지속적인 학습이 이루어지는 동안 모델의 안정성과 성능을 유지하면서 훈련 가능한 파라미터 수를 크게 줄입니다. 결과적으로, CURLoRA는 데이터가 제한된 상황에서도 다양한 작업에서 매우 좋은 안정적인 정확도를 유지하며, 기본 모델의 perplexity 점수도 고정되어 있음을 보여줍니다.

## 쉬운설명:
이 논문은 **대형 언어 모델**을 더 효율적으로 훈련할 수 있는 새로운 방법을 소개합니다. 새로운 접근 방식인 **CURLoRA**는 기존 방법에 비해 **훈련 중 정보 유실(Catastrophic Forgetting)**을 줄이고, 더 적은 수의 파라미터로 모델의 성능을 유지하게 해줍니다. 여러 실험을 통해 CURLoRA가 기존 방법보다 더 나은 성능을 보여줬습니다.

## 관련분야:
- 인공지능 (Artificial Intelligence)
- 자연언어처리 (Natural Language Processing, NLP)
- 기계 학습 (Machine Learning)

## 추천수:
0

## PDF 다운로드 링크:
![PDF 다운로드](https://arxiv.org/pdf/2408.14572)