![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19613.png)
## 제목:
Self-rewarding correction for mathematical reasoning

**요약**:
이 논문은 큰 언어 모델(LLM)이 외부 피드백 없이 추론 과정 중 자신의 출력의 정확성을 평가하고 독립적으로 추론 과정을 이끄는 방법을 연구합니다. 특히, 이 연구는 '자기 수정(self-correction)'이라는 대표적인 작업에 중점을 두고 있으며, 모델이 자신의 응답에서 오류를 감지하고, 출력을 수정하며, 반복적 정제를 중단할 시점을 결정할 수 있게 합니다. 이를 가능하게 하기 위해, 저자들은 두 단계의 알고리즘적 프레임워크를 제안합니다. 첫 번째 단계에서는 데이터 생성 및 정제를 통해 모델이 자기 보상(self-rewarding)과 자기 수정 패턴을 학습하도록 합니다. 두 번째 단계에서는 강화 학습을 활용하여 모델이 응답의 정확성을 평가하고 출력을 정제하는 능력을 향상시킵니다. 실험을 통해 이 방법이 기존의 자기 수정 능력을 능가하며 외부 보상 모델에 의존하는 시스템에 비견될 수 있는 성능을 구현함을 보였습니다.

**쉬운설명**:
이 연구는 AI 모델들이 스스로 오류를 찾아내고 수정하는 방법을 탐구합니다. 외부의 도움 없이 스스로 문제를 해결하는 방법을 연구했으며, 두 가지 단계로 이를 학습시켰습니다. 이 방법을 통해 AI는 더 똑똑해져서 사람의 도움 없이도 스스로 문제를 해결할 수 있습니다.

**관련분야**:
인공지능, 데이터 분석

**추천수**:
38

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2502.19613)
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20082.png)
## 제목:
LongRoPE2: Near-Lossless LLM Context Window Scaling

**요약**:
LongRoPE2는 사전 학습된 큰 언어 모델(LLM)의 컨텍스트 윈도우를 확장하면서도 기존의 성능을 유지하는 새로운 접근법을 제시합니다. 이는 컨텍스트 윈도우가 길어지면서 발생할 수 있는 문제를 해결하기 위해 몇몇 기여가 이루어졌습니다. 첫째, 기존 방법에서 발견된 비정상적 문제를 해결하고 둘째, "needle-driven" perplexity를 통해 고차원 RoPE의 불충분한 훈련 문제를 해결하는 알고리즘을 제안합니다. 셋째, 혼합 컨텍스트 윈도우 훈련 접근법을 활용하여 모델의 가중치를 조정하여 장문 컨텍스트 시퀀스에 대한 학습을 강화합니다. 실험을 통해 LongRoPE2가 기존 방법보다 80배 적은 토큰을 사용하면서도 성능을 유지하는 것을 입증했습니다.

**쉬운설명**:
이 연구는 AI가 문맥을 더 길게 이해할 수 있도록 돕는 방법입니다. AI가 정보를 더 많이 기억하면서도 성능을 잃지 않도록 하는 방법을 찾았습니다. 이를 통해 학습 자원을 효율적으로 사용할 수 있습니다.

**관련분야**:
인공지능, 자연어 처리

**추천수**:
13

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2502.20082)
---