![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02682.png)

## 제목:
MPO: Boosting LLM Agents with Meta Plan Optimization

**요약**:
이 논문에서는 대형 언어 모델(LLMs) 기반의 에이전트가 계획 과제를 수행할 때 흔히 발생하는 문제인 계획 환각(Planning Hallucination)을 해결하기 위한 방법으로 메타 계획 최적화(MPO) 프레임워크를 제안합니다. MPO는 기존 방법들과 달리 복잡한 지식에 의존하지 않으며, 에이전트의 작업 실행 피드백을 기반으로 메타 계획을 지속적으로 최적화합니다. 두 가지 대표 과제에 대한 실험 결과, MPO의 성능이 기존 기준선보다 우수하였으며, 새로운 시나리오에서도 강한 일반화 능력과 효율성을 보였습니다.

**쉬운설명**:
간단히 말하면, 이 논문은 지금까지 사용해 온 계획 방법들이 새로운 에이전트마다 재훈련이 필요하고 잘못된 계획을 보여주는 문제를 해결하기 위해, 에이전트가 필요한 실질적인 가이드를 사용하여 계획을 개선하는 방법을 소개합니다. 이를 통해 에이전트의 작업 성공률을 높이고 더 다양한 환경에서도 잘 작동하도록 만들었습니다.

**관련분야**:
인공지능 계획(Planning in AI), 대형 언어 모델(Large Language Models), 에이전트 기반 시스템(Agent-Based Systems)

**추천수**: 9

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2503.02682)
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02846.png)

## 제목:
Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs

**요약**:
Mask-DPO라는 새로운 방법을 통해 LLM의 응답이 자주 포함하는 비사실적 정보들과 싸우고자 합니다. 이 방법은 세부적인 사실성 정렬을 위해 문장 수준의 사실 신호를 도입합니다. Mask-DPO는 선호하는 샘플의 사실적인 문장만 학습하여 기존의 응답 레벨의 선호도 학습의 모호성을 해결합니다. 결과적으로 다양한 데이터셋에서 질문에 대한 LLM 응답의 사실성을 크게 향상시켰으며, 훈련되지 않은 주제에서도 사실 점수의 증가를 보여주었습니다.

**쉬운설명**:
이 논문은 AI가 잘못된 정보를 줄이는 방법에 대해 설명합니다. 기존에는 전체 답변을 기준으로 AI의 답변을 평가했지만, 이 방법은 문장마다 사실 여부를 확인하여 불필요한 패널티를 줄이고 올바른 정보를 학습하도록 만듭니다. 이를 통해 AI가 주어진 주제에 대해 더 실제하는 답변을 제공할 수 있게 되었습니다.

**관련분야**:
사실성 정렬(Factuality Alignment), 대형 언어 모델(Large Language Models), AI 윤리 및 책임성(Ethics and Accountability in AI)

**추천수**: 9

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2503.02846)
---