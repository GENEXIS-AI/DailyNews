![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png)

## 제목:
**GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization**

**요약**:
다중 보상 강화 학습(Multi-reward Reinforcement Learning)은 보상 정규화가 실패하여 학습 안정성과 성능에 영향을 미친다는 문제가 있습니다. GDPO(Group reward-Decoupled Normalization Policy Optimization)는 보상 정규화를 독립적으로 수행하여 이러한 문제를 해결하고, 다양한 추론 작업에서 성능을 향상시킵니다.

**쉬운설명**:
이 논문은 여러 보상을 다루는 강화 학습에서 보상의 크기가 서로 달라 학습이 어려운 점을 해결하기 위해 보상 크기를 각자 따로 맞춰 주는 방법을 제안합니다. 이를 통해 다양한 문제에서도 더 잘 학습할 수 있습니다.

**관련분야**:
강화 학습, 다중 보상 학습, 강화 학습 안정성

**추천수**:
51

**PDF 다운로드 링크**: [PDF 다운로드](https://arxiv.org/pdf/2601.05242)