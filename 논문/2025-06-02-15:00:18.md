![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24863.png)
## 자료 제목: AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time
**요약**: 이 논문에서는 AlphaOne (alpha1)이라는 대규모 추론 모델(LRM)의 추론 과정을 테스트 시간 동안 조절할 수 있는 보편적 프레임워크를 소개합니다. alpha1은 먼저 "alpha moment"라는 개념을 도입하는데, 이는 보편적인 매개 변수인 alpha로 조정된 생각 단계(scaled thinking phase)를 나타냅니다. alpha moment 이전에는, 추론 전환 토큰(reasoning transition tokens)의 삽임을 베르누이 확률 과정으로 모델링하여 느린 생각의 전환을 동적으로 스케줄합니다. alpha moment 이후에는 느린 생각을 종료토큰을 통해 결정론적으로 종료시켜서 빠른 추론과 효율적인 답변 생성을 촉진합니다. 이 접근 방식은 기존의 단조 확장 방법을 통합하고 일반화하여 유연하고 밀도 높은 느린 생각-빠른 생각 추론 조절을 가능하게 합니다. 수학, 코딩, 과학 분야의 다양한 챌린징 벤치마크에서의 광범위한 경험적 연구를 통해 alpha1의 우수한 추론 능력과 효율성을 입증합니다.
**쉬운설명**: 이것은 AI 모델이 느린 생각과 빠른 생각을 어떻게 잘 활용하여 더 좋은 결과를 낼 수 있는지를 연구한 논문입니다. 새로운 기술을 통해 이 AI 모델이 어떻게 테스트 시간에 더 스마트하게 작동하는지를 설명합니다.
**관련분야**: AI 추론 모델, 인공지능 효율성
**추천수**: 26
**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2505.24863)
---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24864.png)
## 자료 제목: ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models
**요약**: 이 논문은 대규모 언어 모델의 추론 능력을 강화할 수 있는 방법으로 강화 학습(RL)을 활용한 ProRL이라는 접근을 소개합니다. ProRL은 장기간의 강화 학습을 통해 기본 모델로는 접근할 수 없는 새로운 추론 전략을 찾아낼 수 있음을 보여줍니다. 이 방법은 KL 발산 제어, 기준 정책 재설정 및 다양한 과제들을 포함하여 모델 성능을 평가합니다. RL로 훈련된 모델은 기본 모델보다 일관되게 높은 성능을 보입니다.
**쉬운설명**: AI가 더 똑똑해지도록 도와주는 새로운 학습 방법을 제시합니다. 더 오래 연습시키면 더 새로운 방법으로 문제를 풀 수 있도록 한다는 것을 보여줍니다.
**관련분야**: 강화 학습, 대규모 언어 모델
**추천수**: 24
**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2505.24864)
---