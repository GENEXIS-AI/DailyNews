![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png)

## 제목:
**Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space**

**요약**:
이 논문에서는 *Dynamic Large Concept Models (DLCM)*이라고 불리는 새로운 계층적 언어 모델링 프레임워크를 소개합니다. 기존의 *Large Language Models*는 모든 토큰에 동일한 계산을 적용하지만, 이는 언어의 비균일한 정보 밀도를 충분히 활용하지 못합니다. DLCM은 잠재 표현에서 의미 경계를 학습하고, 토큰 대신 압축된 개념 공간에서 계산을 수행하여 더 효율적인 추론을 가능하게 합니다. 이는 가변 길이의 개념을 종단 간(end-to-end)으로 발견하며, 사전 정의된 언어 단위에 의존하지 않습니다. 논문에서는 최초로 압축 인식 스케일링 법칙을 제시하여 계산 리소스를 효과적으로 배분합니다. 
*Zero-shot* *benchmark*에서 평균 2.69%의 성능 향상을 보였습니다.

**쉬운설명**:
이 연구는 언어 모델이 언어의 의미를 더 잘 이해하도록 돕는 새로운 방법을 제안합니다. 주어진 문법이나 구조에 얽매이지 않고, 더 효율적인 이해를 위해 유연하게 개념을 압축하고 이를 바탕으로 학습하는 방법을 설명했습니다. 이 모델은 복잡한 언어 구조를 더 잘 잡아내어 성능을 크게 개선할 수 있습니다.

**관련분야**:
딥러닝, 언어 모델링, 자연어 처리

**추천수**:
10

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2512.24617)

---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22630.png)

## 제목:
**On the Role of Discreteness in Diffusion LLMs**

**요약**:
이 논문에서는 언어 생성에서 확산 모델의 역할과 텍스트 구조에서의 비연속성을 탐구합니다. 확산 모델은 병렬 디코딩과 반복적 수정이라는 점에서 장점을 가지고 있지만, 텍스트의 비연속적이고 구조적인 특성 때문에 확산 원리에 직접적으로 적용하기 어렵습니다. 저자들은 기존 접근 방식을 내재 공간에서의 연속 확산 및 토큰 별 확산으로 분류하며 각각의 차이를 분석합니다. 분석 결과, 일관된 정보 분포를 반영하지 못하는 문제와 다중 토큰 의존성을 캡처할 수 없는 문제점을 발견했습니다. 이를 바탕으로 보다 일관된 언어 확산 모델을 향한 향후 연구를 권장합니다.

**쉬운설명**:
이 연구는 기존의 언어 생성 방법론을 개선하기 위해 새로운 언어 모델링 방법을 소개했습니다. 언어의 구조와 의미를 더 잘 이해할 수 있도록 돕는 방식에 대해 설명하고, 텍스트 생성에서의 어려움과 이를 개선하기 위한 연구 방향을 제시했습니다.

**관련분야**:
자연어 처리, 언어 생성, 기계 학습

**추천수**:
3

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2512.22630)