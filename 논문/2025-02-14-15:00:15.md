![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08910.png)
## 제목:
InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU

**요약**:
이 논문은 대규모 언어 모델(LLMs)의 긴 컨텍스트 처리 문제를 해결하기 위해 InfiniteHiP라는 새로운 LLM 추론 프레임워크를 소개합니다. 이 프레임워크는 모듈식 계층적 토큰 프루닝 알고리즘을 도입하여 비관련 컨텍스트 토큰을 동적으로 제거함으로써 처리 속도를 향상시킵니다. 이를 통해 단일 L40s 48GB GPU에서 최대 300만 개의 토큰을 처리할 수 있습니다. 결과적으로 특별한 훈련 없이도 주어진 컨텍스트에서 1백만 개의 토큰에 대해 18.95배의 속도를 기록하며, GPU 메모리 압박을 크게 줄입니다.

**쉬운설명**:
이 논문은 매우 긴 문장을 이해하는데 어려움을 겪는 대규모 언어 모델을 개선하는 방법을 제안합니다. 기존에는 오랜 시간과 많은 메모리가 필요했지만, 새로운 방법은 필요 없는 데이터를 효율적으로 제거하여 작업을 보다 빠르고 효율적으로 합니다.

**관련분야**: 인공지능, 자연어 처리, 머신러닝

**추천수**: 29

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2502.08910)

---

![Image](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08690.png)
## 제목:
Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation

**요약**:
이 논문은 텍스트-이미지 변환 모델의 텍스트 인코더가 차지하는 메모리 효율성을 향상시키기 위해 "Skrr"라는 새로운 전략을 소개합니다. Skrr은 변형 블록 내의 중복성을 활용하여 특정 레이어를 건너뛰거나 재사용하여 메모리를 줄이면서 성능은 유지합니다. 이를 통해 원본 모델과 동등한 이미지 품질을 유지하면서도 메모리 사용량을 크게 절감할 수 있습니다.

**쉬운설명**:
이 연구에서는 텍스트를 이미지로 변환할 때 너무 많은 메모리가 사용되는 문제를 해결합니다. 필요한 부분만 사용하고, 나머지는 생략해 메모리 걱정을 덜면서도 좋은 이미지를 만들 수 있습니다.

**관련분야**: 인공지능, 비전 인식, 메모리 최적화

**추천수**: 15

**PDF 다운로드 링크**: ![PDF 다운로드](https://arxiv.org/pdf/2502.08690)